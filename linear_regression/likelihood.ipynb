{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a likelihood function? Also add a formula and explain what it means.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The Likelihood function gives us an idea of how well the data summarizes these parameters.The “parameters” here aren’t population parameters— they are the parameters for a particular probability distribution function (PDF). \n",
    " Suppose the joint probability density function of your sample X = (X1,…X2) is f(x| θ), where θ is a parameter. X = x is an observed sample point. Then the function of θ defined as\n",
    "\n",
    "L(θ |x) = f(x |θ)\n",
    "is the likelihood function.\n",
    "\n",
    "Let $ X^n = (X1, · · · , Xn) $ have joint density $p(x^n\n",
    "; θ) = p(x1, . . . , x_n; θ)$ where\n",
    "θ ∈ Θ. The likelihood function L : Θ → [0,∞) is defined by:\n",
    "<p>$L(θ) ≡ L(θ; x^n) = p(x^n; θ)$</p>\n",
    "\n",
    "where $ x^n $ is fixed and θ varies in Θ. The log-likelihood function is \n",
    "<p>$\\ell(\\theta)=\\log L(\\theta)$</p>\n",
    "\n",
    "1. The likelihood function is a function of θ.\n",
    "2. The likelihood function is not a probability density function.\n",
    "3. If the data are iid then the likelihood is\n",
    "<p>$L(\\theta)=\\prod_{i=1}^{n} p\\left(x_{i} ; \\theta\\right)$</p> iid(independent and indentical distribution) case only.\n",
    "4. The likelihood is only defined up to a constant of proportionality. In other words, it is\n",
    "an equivalence class of functions.\n",
    "5. The likelihood function is used (i) to generate estimators (the maximum likelihood\n",
    "estimator) and (ii) as a key ingredient in Bayesian inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  What is Maximum Likelihood estimation (MLE) ? Can you give an example?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum Likelihood is a way to find the most likely function to explain a set of observed data.\n",
    "Maximum Likelihood Estimation is one way to find the parameters of the population that is most likely to have generated the sample being tested. How well the data matches the model is known as “Goodness of Fit.”\n",
    "<p>\n",
    "For example, a researcher might be interested in finding out the mean weight gain of person eating a particular diet. The researcher is unable to weigh every person in the population so instead takes a sample. Weight gains of person tend to follow a normal distribution; Maximum Likelihood Estimation can be used to find the mean and variance of the weight gain in the general population based on this sample.</p>\n",
    "\n",
    "X1, X2, X3, . . . Xn have joint density denoted\n",
    "<p>\n",
    "$f_θ(x1, x2, . . . , xn) = f(x1, x2, . . . , xn|θ)$</p>\n",
    "Given observed values X1 = x1, X2 = x2, . . . , Xn = xn, the likelihood of θ is the function\n",
    "<p>$lik(θ) = f(x1, x2, . . . , xn|θ)$</p>\n",
    "considered as a function of θ.\n",
    "If the distribution is discrete, f will be the frequency distribution function.\n",
    "In words:\n",
    "<b>lik(θ)=probability of observing the given data as a function of θ.</b>\n",
    "\n",
    "If the Xi are iid, then the likelihood simplifies to\n",
    "<p>\n",
    "$\\operatorname{lik}(\\theta)=\\prod_{i=1}^{n} f\\left(x_{i} | \\theta\\right)$</p>\n",
    "\n",
    "Rather than maximising this product which can be quite tedious, we often use the fact\n",
    "that the logarithm is an increasing function so it will be equivalent to maximise the log\n",
    "likelihood:\n",
    "<p>$l(\\theta)=\\sum_{i=1}^{n} \\log \\left(f\\left(x_{i} | \\theta\\right)\\right)$</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is linear regression related to Pytorch and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a linear approach to modelling the relationship between a dependent variable and one or more independent variables. Let X be the independent variable and Y be the dependent variable. We will define a linear relationship between these two variables as follows:\n",
    "$$Y=mX+c$$\n",
    "$m$ is the slope of the line and $c$ is the y intercept.We will use this equation to train our model with a given dataset and predict the value of $Y$ for any given value of $X$. Our challenge is to determine the value of $m$ and $c$, such that the line corresponding to those values is the best fitting line or gives the minimum error.\n",
    "The loss is the error in our predicted value of $m$ and $c$. Our goal is to minimize this error to obtain the most accurate value of $m$ and $c$.\n",
    "To find the loss function we can use pytorch which has built in libraries which minimizes the number of lines of code. \n",
    "This is all these are related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write out MSE loss for linear regression. Could we also use this loss for classification?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mean Squared Error (MSE) is perhaps the simplest and most common loss function, often taught in introductory Machine Learning courses. To calculate the MSE, you take the difference between your model’s predictions and the ground truth, square it, and average it out across the whole dataset.\n",
    "The MSE will never be negative, since we are always squaring the errors. The MSE is formally defined by the following equation:\n",
    "<p>\n",
    "$\\operatorname{MSE}=\\frac{1}{N} \\sum_{i=1}^{N}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}$</p>\n",
    "using the MSE loss makes sense if the assumption that your outputs are a real-valued function of your inputs, with a certain amount of irreducible Gaussian noise, with constant mean and variance. If these assumptions don’t hold true (such as in the context of classification), the MSE loss may not be the best bet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write out the Maximum likelihood Estimation for linear regression. How is this related to the MSE loss for linear regression derived in the last point? Derive the relation between them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write our linear model with Gaussian noise like this:\n",
    "$\\epsilon \\sim N\\left(0, \\sigma^{2}\\right)$\n",
    "<p>\n",
    "$y=\\theta_{1} x+\\theta_{0}+\\epsilon$</p>\n",
    "To apply maximum likelihood, we first need to derive the likelihood function. First, let’s rewrite our model from above as a single conditional distribution given x:\n",
    "$y \\sim N\\left(\\theta_{1} x+\\theta_{0}, \\sigma^{2}\\right)$\n",
    "the equation of a Gaussian distribution’s probability density function, with our linear equation in place of the mean:\n",
    "$f\\left(y | x ; \\theta_{0}, \\theta_{1}, \\sigma^{2}\\right)=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{\\frac{-\\left(y-\\theta_{1} x+\\theta_{0}\\right)^{2}}{2 \\sigma^{2}}}$\n",
    "<p>Each point is independent and identically distributed (iid), so we can write the likelihood function with respect to all of our observed points as the product of each individual probability density.</p>\n",
    "\n",
    "$L_{X}\\left(\\theta_{0}, \\theta_{1}, \\sigma^{2}\\right)=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\prod_{(x, y) \\in X} e^{\\frac{-\\left(y-\\theta_{1} x+\\theta_{0}\\right)^{2}}{2 \\sigma^{2}}}$\n",
    "<p> To make our equation simpler, let’s take the log of our likelihood.</p> \n",
    "$l_{X}\\left(\\theta_{0}, \\theta_{1}, \\sigma^{2}\\right)=\\log \\left[\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\prod_{(x, y) \\in X} e^{\\frac{-\\left(y-\\left(\\theta_{1} x+\\theta_{0}\\right)^{2}\\right.}{2 \\sigma^{2}} )^{2}}\\right]$\n",
    "\n",
    "$=\\log \\left(\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}}\\right)+\\sum_{(x, y) \\in X} \\log \\left(e^{\\frac{-\\left(y-\\theta_{1} x+\\theta_{0}\\right)^{2}}{2 \\sigma^{2}}}\\right)$\n",
    "\n",
    "$=\\log \\left(\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}}\\right)+\\sum_{(x, y) \\in X} \\frac{-\\left(y-\\left(\\theta_{1} x+\\theta_{0}\\right)\\right)^{2}}{2 \\sigma^{2}}$\n",
    "\n",
    "$=\\log (1)-\\log \\left(\\sqrt{2 \\pi \\sigma^{2}}\\right)-\\frac{1}{2 \\sigma^{2}} \\sum_{(x, y) \\in X}\\left[y-\\left(\\theta_{1} x+\\theta_{0}\\right)\\right]^{2}$\n",
    "\n",
    "$=-\\log \\left(\\sqrt{2 \\pi \\sigma^{2}}\\right)-\\frac{1}{2 \\sigma^{2}} \\sum_{(x, y) \\in X}\\left[y-\\left(\\theta_{1} x+\\theta_{0}\\right)\\right]^{2}$\n",
    "\n",
    "<p>maximizing a number is the same thing as minimizing the negative of the number. So instead of maximizing the likelihood, let’s minimize the negative log-likelihood:</p>\n",
    "$-l_{X}\\left(\\theta_{0}, \\theta_{1}, \\sigma^{2}\\right)=\\log \\left(\\sqrt{2 \\pi \\sigma^{2}}\\right)+\\frac{1}{2 \\sigma^{2}} \\sum(y-\\hat{y})^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write out the likelihood function for linear classification. What is the drawback of using MSE loss here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " the logit model the output variable  $y_{i}$ is a Bernoulli random variable (it can take only two values, either 1 or 0) \n",
    "<p> $\\mathrm{P}\\left(y_{i}=1 | x_{i}\\right)=S\\left(x_{i} \\beta\\right)$</p>\n",
    "where\n",
    "$S(t)=\\frac{1}{1+\\exp (-t)}$ \n",
    "is the logistic function,  \n",
    "$x_{i}$\n",
    "is a \n",
    "$1xK$ vector of inputs and  \n",
    "$\\beta $\n",
    "is a  Kx1 vector of coefficients.\n",
    "\n",
    "Furthermore,\n",
    "the likelihood of the entire sample is equal to the product of the likelihoods of the single observations:\n",
    "$L\\left(\\beta ; y_{i}, x_{i}\\right)=\\left[S\\left(x_{i} \\beta\\right)\\right]^{y_{i}}\\left[1-S\\left(x_{i} \\beta\\right)\\right]^{1-y_{i}}$\n",
    "\n",
    "$\\begin{aligned} l(\\beta ; y, X) &=\\ln (L(\\beta ; y, X)) \\\\ &=\\ln \\left(\\prod_{i=1}^{N}\\left[S\\left(x_{i} \\beta\\right)\\right]^{y_{i}}\\left[1-S\\left(x_{i} \\beta\\right)\\right]^{1-y_{i}}\\right) \\\\ &=\\sum_{i=1}^{N}\\left[y_{i} \\ln \\left(S\\left(x_{i} \\beta\\right)\\right)+\\left(1-y_{i}\\right) \\ln \\left(1-S\\left(x_{i} \\beta\\right)\\right)\\right] \\end{aligned}$   \n",
    " \n",
    "$\\begin{array}\n",
    "{=\\sum_{i=1}^{N}\\left[y_{i} \\ln \\left(\\frac{1}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)+\\left(1-y_{i}\\right) \\ln \\left(1-\\frac{1}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)\\right]} \\\\ \n",
    "{=\\sum_{i=1}^{N}\\left[y_{i} \\ln \\left(\\frac{1}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)+\\left(1-y_{i}\\right) \\ln \\left(\\frac{1+\\exp \\left(-x_{i} \\beta\\right)-1}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)\\right]} \\\\ \n",
    "{=\\sum_{i=1}^{N}\\left[y_{i} \\ln \\left(\\frac{1}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)+\\left(1-y_{i}\\right) \\ln \\left(\\frac{\\exp \\left(-x_{i} \\beta\\right)}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)\\right]}\n",
    "\\end{array}$\n",
    "\n",
    "$\n",
    "\\begin{array}{l}{=\\sum_{i=1}^{N}\\left[\\ln \\left(\\frac{\\exp \\left(-x_{i} \\beta\\right)}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)+y_{i}\\left(\\ln \\left(\\frac{1}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)-\\ln \\left(\\frac{\\exp \\left(-x_{i} \\beta\\right)}{1+\\exp \\left(-x_{i} \\beta\\right)}\\right)\\right)\\right]} \\\\ {=\\sum_{i=1}^{N}\\left[\\ln \\left(\\frac{\\exp \\left(-x_{i} \\beta\\right)}{1+\\exp \\left(-x_{i} \\beta\\right)} \\frac{\\exp \\left(x_{i} \\beta\\right)}{\\exp \\left(x_{i} \\beta\\right)}\\right)+y_{i}\\left(\\ln \\left(\\frac{1}{1+\\exp \\left(-x_{i} \\beta\\right)} \\frac{1+\\exp \\left(-x_{i} \\beta\\right)}{\\exp \\left(-x_{i} \\beta\\right)}\\right)\\right\\}\\right]} \\\\ {\\quad=\\sum_{i=1}^{N}\\left[\\ln \\left(\\frac{1}{1+\\exp \\left(x_{i} \\beta\\right)}\\right)+y_{i}\\left(\\ln \\left(\\frac{1}{\\exp \\left(-x_{i} \\beta\\right)}\\right)\\right)\\right]}\\end{array}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{array}{l}{=\\sum_{i=1}^{N}\\left[\\ln (1)-\\ln \\left(1+\\exp \\left(x_{i} \\beta\\right)\\right)+y_{i}\\left(\\ln (1)-\\ln \\left(\\exp \\left(-x_{i} \\beta\\right)\\right)\\right]\\right.} \\\\ {=\\sum_{i=1}^{N}\\left[-\\ln \\left(1+\\exp \\left(x_{i} \\beta\\right)\\right)+y_{i} x_{i} \\beta\\right]}\\end{array}\n",
    "$\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can gradient descent be used to find the parameters for linear regression? What about linear classification? Why? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is used in Linear regression to find the optimized parameters. and is used to update the values of weights using gradient Descent algorithm.\n",
    "\n",
    "In linear classification it is hard to find the paramaters and optimize the cost funcrion using Gradient Descent becausethe boundaries are not clear until it reaches the data point as there is no local information on the point in which it is moving. Hence we have algorithm to estimate parametes silimar to this as regression i.e Perceptron Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are normal equations? Is it the same as least squares? Explain. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Equation is an analytical approach to Linear Regression with a Least Square Cost Function. We can directly find out the value of θ without using Gradient Descent. Following this approach is an effective and a time-saving option when are working with a dataset with small features.\n",
    "\n",
    "Normal Equation is a follows :\n",
    "\n",
    "The \"Normal Equation\" is a method of finding the optimum theta without iteration.\n",
    "\n",
    "$\\theta = (X^T X)^{-1}X^T \n",
    " y$\n",
    "\n",
    "There is no need to do feature scaling with the normal equation.\n",
    "\n",
    "In the above equation,\n",
    "\n",
    "θ : hypothesis parameters that define it the best.\n",
    "\n",
    "X : Input feature value of each instance.\n",
    "\n",
    "Y : Output value of each instance.\n",
    "\n",
    "IS normal equation and least squares method same?\n",
    "\n",
    "Least Square Method:\n",
    "\n",
    "The least squares method is a form of mathematical regression analysis that finds the line of best fit for a set of data, providing a visual demonstration of the relationship between the data points. Each point of data is representative of the relationship between a known independent variable and an unknown dependent variable.\n",
    "\n",
    "Are they same? No they are not same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is feature scaling needed for linear regression when using gradient descent?  Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling of features can be very helpful.It can help in the faster convergence of the algorithm in case you are using Gradient Descent.\n",
    "It can make the analysis of coefficients easier.If your features differ in scale then this may impact the resultant coefficients of the model and it can be hard to interpret the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write out the MLE approach for logistic regression. How is this related to the binary cross-entropy? You can use this reference to answer this question. Video Reference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, to find the maximum likelihood estimates we’d differentiate the log\n",
    "likelihood with respect to the parameters, set the derivatives equal to zero, and solve.\n",
    "\n",
    " \n",
    "Now let’s consider the logistic model (a binary classifier) to describe log-odds using a linear model:\n",
    "\n",
    "$$\\ln \\frac{p}{1-p}=\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{m} x_{m}$$\n",
    "\n",
    "The probability of observing outcome y=1 under this model is given by the following function (sigmoid):\n",
    "\n",
    "$$p \\equiv p(y=1 | \\mathbf{B}, \\mathbf{X})=\\frac{1}{1+e^{-\\left(\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{m} x_{m}\\right)}}$$\n",
    "\n",
    "With 0 and 1 being the only possible outcomes, the probability of observing outcome y=0 is simply (1-p):\n",
    "\n",
    "$$p(y=o | \\mathbf{B}, \\mathbf{X})=p^{o}(1-p)^{1-o}$$\n",
    "The likelihood function is given by the product of all individual probabilities:\n",
    "\n",
    "$$\\mathcal{L}=\\prod_{i=1}^{n} p\\left(y=y_{i} | \\mathbf{B}_{i}, \\mathbf{X}_{i}\\right)=\\prod_{i=1}^{n} p_{i}^{y_{i}}\\left(1-p_{i}\\right)^{1-y_{i}}$$\n",
    "It’s easier to maximize the log-likelihood:\n",
    "\n",
    "$$\\ln \\mathcal{L}=\\sum_{i=1}^{n}\\left(y_{i} \\ln p_{i}+\\left(1-y_{i}\\right) \\ln \\left(1-p_{i}\\right)\\right)$$\n",
    "Thus maximum liklihood estimation yields a familiar loss function (cross-entropy in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
